{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7523fbf",
   "metadata": {},
   "source": [
    "## Loading data... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6004862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from user-text.json\n",
      "CSV read successfully\n",
      "Now creating a dataframe\n",
      "                      title      text tags                   timestamp  \\\n",
      "0  Re: second post of today  my reply   []  2025-03-12T15:34:35.011895   \n",
      "\n",
      "                                     id parent_id sentiment toxicity flags  \n",
      "0  d04a146d-ae09-4ab0-aebe-67a565f6ac5c       NaN       NaN      NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv \n",
    "\n",
    "path = \"user-text.json\"\n",
    "print(\"Loading data from\", path)\n",
    "\n",
    "with open(path) as jf:\n",
    "    d = json.load(jf)\n",
    "\n",
    "entries = d['entries']\n",
    "df = open('data_file.csv', 'w')\n",
    "cw = csv.writer(df)\n",
    "\n",
    "labels = ['title', 'text', 'tags', 'timestamp', 'id', 'parent_id', 'sentiment', 'toxicity', 'flags']\n",
    "labeled = False \n",
    "\n",
    "for i in entries:\n",
    "    for label in labels:\n",
    "        if label not in i: \n",
    "            i[label] = None \n",
    "\n",
    "    if not labeled:\n",
    "        h = i.keys()\n",
    "        cw.writerow(h) # write labels of columns\n",
    "        labeled = True \n",
    "    #cleaning each entry to remove spaces or weird formating \n",
    "    cleaned_entry = {}\n",
    "    for k, v in i.items():\n",
    "        cleaned_value = str(v).replace(\"\\n\", \" \").rstrip()\n",
    "        cleaned_entry[k] = cleaned_value\n",
    "\n",
    "    cw.writerow(cleaned_entry.values()) # write to the file \n",
    "\n",
    "df.close()\n",
    "print(\"CSV read successfully\")\n",
    "\n",
    "print(\"Now creating a dataframe\")\n",
    "df = pd.read_csv(\"data_file.csv\")\n",
    "print(df.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de954e",
   "metadata": {},
   "source": [
    "## Cleaning and Creating the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c88f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              second post today reply\n",
       "1                     response phoene replying dsfadsf\n",
       "2                                             replying\n",
       "3             testing replies trying test reply button\n",
       "4    testing replies replying trying test reply button\n",
       "Name: cleaned_content, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import string \n",
    "\n",
    " #combining title and text into another column \n",
    "df['content'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
    "\n",
    "def clean(text):\n",
    "    if pd.issnull(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    # text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    regex_pattern = f\"[{re.escape(string.punctuation)}]\"\n",
    "    text = re.sub(regex_pattern, \"\", text)\n",
    "    text =  re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = [word for word in text.split() if word not in ENGLISH_STOP_WORDS]\n",
    "    return ' '.join(tokens) \n",
    "\n",
    "df['cleaned_content'] = df['content'].apply(clean)\n",
    "df['cleaned_content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38adf7a",
   "metadata": {},
   "source": [
    "## Applying Non-negative Matrix for Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36c4aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing TFIDF matrix... \n",
      "Fitting the NMF model with n_samples=2000 and n_features=1000...\n",
      "done in 0.052s.\n",
      "Topic #0:\n",
      "testing keywords keywords keywords site site ass site testing ass keywords fuck fuck fuck ass moderation hello fuck site hello fuck testing moderation fuck fuck hello jhgkhj ass jhgkhj modertaaiondf bad modertaaiondf\n",
      "\n",
      "Topic #1:\n",
      "fuck fuck fuck fuck ass moderation ass dont dont like moderation test test fuck dont like ass test keywords fuck testing testing moderation hello fuck fuck site moderation hello hello fucking ass\n",
      "\n",
      "Topic #2:\n",
      "testing test area textarea input test area textarea text textarea input body text text body area replying replying body test testing area body replying text body hello asking medical asking\n",
      "\n",
      "Topic #3:\n",
      "blah words fuck bad words bad words ass bad blah ass blah blah adslkfjdsf adslkfjdsf blah fuck ass modertaaiondf modertaaiondf bad testing modertaaiondf fuck ass testing fuck fucking fucking fucking ass\n",
      "\n",
      "Topic #4:\n",
      "replies testing replies reply test reply trying test reply button button testing trying replies trying test replying trying replies replying replying second today reply post post today second post today\n",
      "\n",
      "Topic #5:\n",
      "body tag mapping tag mapping test test body text body text body body test tag mapping body mapping test entry body entry mapp test tag mapp mapp text body map work tag map\n",
      "\n",
      "Topic #6:\n",
      "let adoptee feel symposium chinese adoptee fantasy chinese doesnt explain know doesnt allowed day make explain research adoption adoptee symposium try friends ive\n",
      "\n",
      "Topic #7:\n",
      "test ass ass test ass fetish fetish jhgkhj jhgkhj ass fuck fucking fucking fucking ass aadsfdsf aadsfdsf fuck mapp test mapp tag mapp test body fuck trying tag test trying tag map\n",
      "\n",
      "Topic #8:\n",
      "popup testing tag tag popup tag testing popup tkjdkjkajdsfkjdskfj tkjdkjkajdsfkjdskfj popup bkjskdjfsjdf bkjskdjfsjdf testing popup jkjfjfjjf popup jkjfjfjjf map work test trying trying tag tag map work map mapping test trying\n",
      "\n",
      "Topic #9:\n",
      "replying jdjfkdaldkfj replying jdjfkdaldkfj undefined undefined replying dsfadsf response phoene replying dsfadsf phoene replying phoene response replying trying replies replying area replying replying body test reply trying test button reply button trying\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from time import time\n",
    "\n",
    "\n",
    "#Creating Vectors from tokens \n",
    "#initialzing vectorizer, can change different parameters such as \n",
    "#how many features or how many words features contain\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "t0 = time()\n",
    "tokens = df['cleaned_content']\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer(max_features = n_features, ngram_range=(1,2))\n",
    "\n",
    "X = vectorizer.fit_transform(tokens)\n",
    "\n",
    "features = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns = features)\n",
    "\n",
    "print(\"Showing TFIDF matrix... \")\n",
    "tfidf_df.head()\n",
    "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "nmf = NMF(n_components = n_topics, random_state = 1).fit(tfidf_df)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafaa7c3",
   "metadata": {},
   "source": [
    "## Using results for tag mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5fa56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
